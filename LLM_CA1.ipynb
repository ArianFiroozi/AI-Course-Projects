{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ArianFiroozi/AI-Course-Projects/blob/master/LLM_CA1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f3p_QiOCftKB"
      },
      "source": [
        "## CA 1, LLMs Spring 2025\n",
        "\n",
        "- **Name:** Arian Firoozi\n",
        "- **Student ID:** 810100196\n",
        "\n",
        "---\n",
        "#### Your submission should be named using the following format: `CA1_LASTNAME_STUDENTID.ipynb`.\n",
        "\n",
        "---\n",
        "\n",
        "##### *How to do this problem set:*\n",
        "\n",
        "- Some questions require writing Python code and computing results, and the rest of them have written answers. For coding problems, you will have to fill out all code blocks that say `YOUR CODE HERE`.\n",
        "\n",
        "- For text-based answers, you should replace the text that says ```Your Answer Here``` with your actual answer.\n",
        "\n",
        "- There is no penalty for using AI assistance on this homework as long as you fully disclose it in the final cell of this notebook (this includes storing any prompts that you feed to large language models). That said, anyone caught using AI assistance without proper disclosure will receive a zero on the assignment (we have several automatic tools to detect such cases). We're literally allowing you to use it with no limitations, so there is no reason to lie!\n",
        "\n",
        "---\n",
        "\n",
        "##### *Academic honesty*\n",
        "\n",
        "- We will audit the Colab notebooks from a set number of students, chosen at random. The audits will check that the code you wrote actually generates the answers in your notebook. If you turn in correct answers on your notebook without code that actually generates those answers, we will consider this a serious case of cheating.\n",
        "\n",
        "- We will also run automatic checks of Colab notebooks for plagiarism. Copying code from others is also considered a serious case of cheating.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GZvMUZ70ftKD"
      },
      "source": [
        "If you have any further questions or concerns, contact the TAs via email: vahyd@live.com / amirh.bonakdar@ut.ac.ir"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-03-03T16:53:55.495413Z",
          "iopub.status.busy": "2025-03-03T16:53:55.495044Z",
          "iopub.status.idle": "2025-03-03T16:53:58.976676Z",
          "shell.execute_reply": "2025-03-03T16:53:58.975564Z",
          "shell.execute_reply.started": "2025-03-03T16:53:55.495388Z"
        },
        "id": "KTNNJLLaS1bL",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "!pip install transformers peft datasets accelerate scipy bitsandbytes wandb  -qqq"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"hello world\") #testing github api"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wDERRvmLngYF",
        "outputId": "5ab113f9-607d-4425-a4a2-6008dd32d8b8"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hello world\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yedtpwo3qP-j"
      },
      "source": [
        "### Q0: Setting Up"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JHjJb4kRTXOq"
      },
      "source": [
        "Create a Huggingface Access Token From:\n",
        "https://huggingface.co/settings/tokens\n",
        "\n",
        "You need to request for access to:\n",
        "- ```meta-llama/Llama-3.2-1B```\n",
        "- ```meta-llama/Llama-3.2-1B-Instruct```\n",
        "- ```mistralai/Mistral-7B-v0.1```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2025-03-03T16:40:02.134908Z",
          "iopub.status.busy": "2025-03-03T16:40:02.134588Z",
          "iopub.status.idle": "2025-03-03T16:40:02.896365Z",
          "shell.execute_reply": "2025-03-03T16:40:02.895488Z",
          "shell.execute_reply.started": "2025-03-03T16:40:02.134885Z"
        },
        "id": "s-SGTuMBTCC-",
        "outputId": "d8c44edc-2434-424c-ff0f-e577c7837a36",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
            "Token is valid (permission: fineGrained).\n",
            "The token `llm_ut_ca1` has been saved to /root/.cache/huggingface/stored_tokens\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful.\n",
            "The current active token is: `llm_ut_ca1`\n"
          ]
        }
      ],
      "source": [
        "!huggingface-cli login --token {YOUR_HF_TOKEN}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-03-03T16:40:02.898294Z",
          "iopub.status.busy": "2025-03-03T16:40:02.898064Z",
          "iopub.status.idle": "2025-03-03T16:40:02.902973Z",
          "shell.execute_reply": "2025-03-03T16:40:02.902087Z",
          "shell.execute_reply.started": "2025-03-03T16:40:02.898275Z"
        },
        "id": "HPUo2WODR70K",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import transformers\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from peft import LoraConfig, TaskType, get_peft_model, PeftModel, PrefixTuningConfig, PromptTuningConfig\n",
        "import os\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-03-03T16:40:02.904515Z",
          "iopub.status.busy": "2025-03-03T16:40:02.904237Z",
          "iopub.status.idle": "2025-03-03T16:40:02.915012Z",
          "shell.execute_reply": "2025-03-03T16:40:02.914250Z",
          "shell.execute_reply.started": "2025-03-03T16:40:02.904483Z"
        },
        "id": "bRSAkaZkR70K",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "BASE_MODEL = 'meta-llama/Llama-3.2-1B'\n",
        "INSTRUCT_MODEL = 'meta-llama/Llama-3.2-1B-Instruct'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2025-03-03T16:40:02.916188Z",
          "iopub.status.busy": "2025-03-03T16:40:02.915923Z",
          "iopub.status.idle": "2025-03-03T16:40:02.930609Z",
          "shell.execute_reply": "2025-03-03T16:40:02.929922Z",
          "shell.execute_reply.started": "2025-03-03T16:40:02.916170Z"
        },
        "id": "C0M2ECOW8Mc_",
        "outputId": "c2088b05-c3c3-4011-af56-f497ed7aa106",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "DEVICE = \"cpu\"\n",
        "if torch.backends.mps.is_available():\n",
        "    DEVICE = \"mps\"\n",
        "elif torch.cuda.is_available():\n",
        "    DEVICE = \"cuda\"\n",
        "\n",
        "print(f\"Using device: {DEVICE}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bI2ni6n3R70K"
      },
      "source": [
        "## Getting Started with LLMs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "779rMkF7R70L"
      },
      "source": [
        "## Q1: First Steps (25 pts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RT_E0pya8Mc_"
      },
      "source": [
        "The outputs of tokenizer are not human readable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2025-03-03T16:40:02.931780Z",
          "iopub.status.busy": "2025-03-03T16:40:02.931505Z",
          "iopub.status.idle": "2025-03-03T16:40:07.876428Z",
          "shell.execute_reply": "2025-03-03T16:40:07.875785Z",
          "shell.execute_reply.started": "2025-03-03T16:40:02.931754Z"
        },
        "id": "-nrrL6rtR70L",
        "outputId": "ddbc34c4-67df-4b2a-904e-0c38ea75c28b",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "model_id = INSTRUCT_MODEL\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    device_map=DEVICE,\n",
        ")\n",
        "tokenizer.pad_token = tokenizer.eos_token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2025-03-03T16:40:07.877340Z",
          "iopub.status.busy": "2025-03-03T16:40:07.877138Z",
          "iopub.status.idle": "2025-03-03T16:40:08.396365Z",
          "shell.execute_reply": "2025-03-03T16:40:08.395549Z",
          "shell.execute_reply.started": "2025-03-03T16:40:07.877313Z"
        },
        "id": "opmbKj87R70M",
        "outputId": "dea1e5c8-3cfa-4ad6-a1fc-d3f9e73ddfdc",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        }
      ],
      "source": [
        "prompt = \"What is 2 plus 2?\"\n",
        "\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\").to(DEVICE)\n",
        "\n",
        "outputs = model.generate(\n",
        "    input_ids=inputs[\"input_ids\"],\n",
        "    attention_mask=inputs[\"attention_mask\"],\n",
        ")\n",
        "\n",
        "outputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GLpzl0a18MdA"
      },
      "source": [
        "#### Q1.1: Readable Model Generation (1 pts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sYmkV1hsR70M"
      },
      "source": [
        "a. As you see the model outputs token ids which are not readable to us. We should decode this to human readable language. Using the ```decode``` function on the tokenizer, print the human readable model generation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-03-03T16:40:08.398688Z",
          "iopub.status.busy": "2025-03-03T16:40:08.398477Z",
          "iopub.status.idle": "2025-03-03T16:40:08.401814Z",
          "shell.execute_reply": "2025-03-03T16:40:08.401022Z",
          "shell.execute_reply.started": "2025-03-03T16:40:08.398670Z"
        },
        "id": "4IX2xFY_8MdA",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "### Your Code Here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MSa4KW9-R70N"
      },
      "source": [
        "b. The input prompt is still a part of the output, but we only want to see the model generation. Fix this problem."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-03-03T16:40:08.415944Z",
          "iopub.status.busy": "2025-03-03T16:40:08.415691Z",
          "iopub.status.idle": "2025-03-03T16:40:08.426432Z",
          "shell.execute_reply": "2025-03-03T16:40:08.425597Z",
          "shell.execute_reply.started": "2025-03-03T16:40:08.415914Z"
        },
        "id": "USDrnEAYR70N",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "### Your Code Here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wmVDaSDmR70P"
      },
      "source": [
        "#### Q1.2: Generation Function (1 pts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XDyXUF2C8MdA"
      },
      "source": [
        "a. Write and test a function that takes the model, generation config as kwargs with default values, tokenizer and prompt as input and outputs the model generation (generation only). You will be using this in the next sections quite a lot."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-03-03T16:40:08.427488Z",
          "iopub.status.busy": "2025-03-03T16:40:08.427273Z",
          "iopub.status.idle": "2025-03-03T16:40:09.563036Z",
          "shell.execute_reply": "2025-03-03T16:40:09.562301Z",
          "shell.execute_reply.started": "2025-03-03T16:40:08.427471Z"
        },
        "id": "pX9Izr0FR70P",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "## Your Code Here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ora8C1Dv8MdA"
      },
      "source": [
        "#### Q1.3: Comparing different Tokenizers (3 pts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RTgi9lp18MdA"
      },
      "source": [
        "a. Bring in the tokenizer for:\n",
        "\n",
        "- ```meta-llama/Llama-3.2-1B```\n",
        "- ```mistralai/Mistral-7B-v0.1```\n",
        "- ```microsoft/Phi-4-mini-instruct```.\n",
        "\n",
        "Tokenize a PERSIAN sentence with at least 10 words using the tokenizers of all three models from different families and print the human readable output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-03-03T16:40:09.563962Z",
          "iopub.status.busy": "2025-03-03T16:40:09.563773Z",
          "iopub.status.idle": "2025-03-03T16:40:09.567247Z",
          "shell.execute_reply": "2025-03-03T16:40:09.566519Z",
          "shell.execute_reply.started": "2025-03-03T16:40:09.563946Z"
        },
        "id": "8HRpntnh8MdA",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "### Your Code Here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZMUGnuUzr8M8"
      },
      "source": [
        "b. Compare the outputs, Which one produces better tokens? What is the reason for this difference in tokenization?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "23IF6J__sEgH"
      },
      "source": [
        "```Your Answer Here```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9WaP1Ril8MdA"
      },
      "source": [
        "#### Q1.4: Base Model vs. Instruction-tuned Model (10 pts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z7IrKIjzR70P"
      },
      "source": [
        "a. See the difference between Base and Instruct Models using the prompt ```What is 2+2?```, Keep in mind that when temperature != 0, you will get different answers. Generate the answers a few time to get a sense of how models work.\n",
        "\n",
        "***NOTE:*** It is recommended to play with various prompts and generation configs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-03-03T16:40:09.568183Z",
          "iopub.status.busy": "2025-03-03T16:40:09.567898Z",
          "iopub.status.idle": "2025-03-03T16:40:20.286750Z",
          "shell.execute_reply": "2025-03-03T16:40:20.285934Z",
          "shell.execute_reply.started": "2025-03-03T16:40:09.568162Z"
        },
        "id": "N2xvr1L8R70R",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "## Your Code Here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IW0Dj72VsP-d"
      },
      "source": [
        "b. In a concise way, what is the difference in outputs? Why the models answer the way they do and how does it affect the way we prompt them?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AFiPRBdOsXGM"
      },
      "source": [
        "```Your Answer Here```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vkSaGiFSR70R"
      },
      "source": [
        "#### Q1.5: Chat Templates for Instruct Models (10 pts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P5wdznhWR70R"
      },
      "source": [
        "When using multi-turn or complex chats with LLMs, to maintain context and keep the generation controlled, it is a good practice to comply with the instruction format used by models. Previous instruction-tuned models needed this to do even the simplest tasks but the recent ones are mostly robust to it and can work without it in simple examples. In this section we will go over this concept.\n",
        "\n",
        "\n",
        "An Instruction (Chat) template generally has 3+1 main components (roles):\n",
        "- System Instruction aka system role\n",
        "- User Query aka user role\n",
        "- LLM Answer aka assistant role\n",
        "- (Tool Calls)\n",
        "\n",
        "```apply_chat_template``` on huggingface tokenizers is a unified interface for chat templates used by different models. The providers are responsible for defining this on the tokenizer according to the template they have used during training stage."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hDS6ESF48MdB"
      },
      "source": [
        "a. Bring in the tokenizer and print the ```chat_template``` property on it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-03-03T16:40:20.287985Z",
          "iopub.status.busy": "2025-03-03T16:40:20.287703Z",
          "iopub.status.idle": "2025-03-03T16:40:20.291379Z",
          "shell.execute_reply": "2025-03-03T16:40:20.290466Z",
          "shell.execute_reply.started": "2025-03-03T16:40:20.287963Z"
        },
        "id": "nEXFQ8fo8MdB",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "### Your Answer Here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ttuXeUP6tCUj"
      },
      "source": [
        "b. In maximum two sentences, what do you see and what is this? How it is used?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mY1RiPHItLEm"
      },
      "source": [
        "```Your Answer Here```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AVTjYdB38MdB"
      },
      "source": [
        "c. Organize the content below using system and user prompt in standard ```ChatML``` format (list of dicts with certain keys), transform them to the instruction format used by LLaMa 3 Models using the ```apply_chat_template``` function and print the human readable output.\n",
        "\n",
        "**System:** You are a funny math teacher, you should answer math questions in a playful and funny tone.\n",
        "\n",
        "**User:** What is 2+2\n",
        "\n",
        "***NOTE:*** You can use ```skip_special_tokens = True``` when decoding to get rid of template tags. You also may update the generate function from previous steps and use that."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-03-03T16:40:20.310519Z",
          "iopub.status.busy": "2025-03-03T16:40:20.310262Z",
          "iopub.status.idle": "2025-03-03T16:40:20.321499Z",
          "shell.execute_reply": "2025-03-03T16:40:20.320800Z",
          "shell.execute_reply.started": "2025-03-03T16:40:20.310488Z"
        },
        "id": "xrKcEhNd8MdB",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "### Your Code Here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1vRG32L88MdB"
      },
      "source": [
        "d. Now prompt the model with and without chat template being applied. (In second scenario simply put the system prompt followed by a newline and the user querry as one single string)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-03-03T16:40:20.322654Z",
          "iopub.status.busy": "2025-03-03T16:40:20.322377Z",
          "iopub.status.idle": "2025-03-03T16:40:20.332588Z",
          "shell.execute_reply": "2025-03-03T16:40:20.331773Z",
          "shell.execute_reply.started": "2025-03-03T16:40:20.322627Z"
        },
        "id": "Yz7txV3R8MdB",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "### Your Code Here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jz3SPkkt8MdB"
      },
      "source": [
        "e. What is your observation, do we need instruction formats in this scenario or the model can follow?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PSEix7zSuGdI"
      },
      "source": [
        "```Your Answer Here```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rZqiQWiK8MdC"
      },
      "source": [
        "Well, let's go a step further :)\n",
        "\n",
        "Below is a conversation between Dr. Yaghoobzadeh and Dr. Dousti (The content is generated by LLMs and I don't know what's going on in this conversation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-03-03T16:40:31.509380Z",
          "iopub.status.busy": "2025-03-03T16:40:31.509143Z",
          "iopub.status.idle": "2025-03-03T16:40:31.513923Z",
          "shell.execute_reply": "2025-03-03T16:40:31.513007Z",
          "shell.execute_reply.started": "2025-03-03T16:40:31.509345Z"
        },
        "id": "oXGcaS_QR70R",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Conversation data ready for applying ChatML template\n",
        "messages = [\n",
        "    {\n",
        "        \"role\": \"system\",\n",
        "        \"content\": \"You are a system that can simulate two AI experts debating: Dr. Dousti (NLP researcher) and Dr. Yaghoobzadeh (NLP researcher). When I write [DOUSTI], respond as Dr. Dousti. When I write [YAGHOOBZADEH], respond as Dr. Yaghoobzadeh. Both should maintain their distinct perspectives and expertise.\"\n",
        "    },\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": \"[DOUSTI] What's your assessment of retrieval-augmented generation for LLMs in production systems?\"\n",
        "    },\n",
        "    {\n",
        "        \"role\": \"assistant\",\n",
        "        \"content\": \"As Dr. Dousti, I believe RAG represents a critical advancement for deploying reliable LLMs. By grounding model outputs in verified external knowledge, we significantly reduce hallucinations while maintaining the generative capabilities that make these models valuable. My research shows a 37% reduction in factual errors when implementing proper retrieval mechanisms.\"\n",
        "    },\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": \"[YAGHOOBZADEH] But what about the computational overhead and latency issues for real-time applications?\"\n",
        "    },\n",
        "    {\n",
        "        \"role\": \"assistant\",\n",
        "        \"content\": \"As Dr. Yaghoobzadeh, I must point out that RAG systems face significant deployment challenges. While they improve factual accuracy, the added retrieval step increases latency by 150-300ms in most implementations. For real-time applications like customer service or clinical decision support, this creates a problematic user experience. Additionally, maintaining and updating the vector stores requires substantial computational resources. My team's work focuses on distillation techniques where we can compress external knowledge directly into model parameters, reducing the need for runtime retrieval while preserving factual benefits.\"\n",
        "    },\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": \"[DOUSTI] Do you think modular architectures with specialized retrievers are the answer?\"\n",
        "    }\n",
        "]\n",
        "\n",
        "messages_raw =\"\"\"\n",
        "You are a system that can simulate two AI experts debating: Dr. Dousti (NLP Researcher) and Dr. Yaghoobzadeh (NLP Researcher). When I write [DOUSTI], respond as Dr. Dousti. When I write [YAGHOOBZADEH], respond as Dr. Yaghoobzadeh. Both should maintain their distinct perspectives and expertise.\n",
        "[DOUSTI] What's your assessment of retrieval-augmented generation for LLMs in production systems?\n",
        "As Dr. Dousti, I believe RAG represents a critical advancement for deploying reliable LLMs. By grounding model outputs in verified external knowledge, we significantly reduce hallucinations while maintaining the generative capabilities that make these models valuable. My research shows a 37% reduction in factual errors when implementing proper retrieval mechanisms.\n",
        "[YAGHOOBZADEH] But what about the computational overhead and latency issues for real-time applications?\n",
        "As Dr. Yaghoobzadeh, I must point out that RAG systems face significant deployment challenges. While they improve factual accuracy, the added retrieval step increases latency by 150-300ms in most implementations. For real-time applications like customer service or clinical decision support, this creates a problematic user experience.\n",
        "[DOUSTI] Do you think modular architectures with specialized retrievers are the answer?\n",
        "\"\"\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i-y3O5jx8MdC"
      },
      "source": [
        "f. Now repeat what you have done with funny teacher example and compare the results with and without applying chat template."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-03-03T16:40:31.514852Z",
          "iopub.status.busy": "2025-03-03T16:40:31.514652Z",
          "iopub.status.idle": "2025-03-03T16:40:31.536707Z",
          "shell.execute_reply": "2025-03-03T16:40:31.535864Z",
          "shell.execute_reply.started": "2025-03-03T16:40:31.514835Z"
        },
        "id": "Tp3pu-LY8MdC",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "### Your Code Here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2FsqlbHp8MdD"
      },
      "source": [
        "g. Write your observations down here. Does the model comply to what we want without using templates in this scenario? Why?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "apAhGe2fuqPz"
      },
      "source": [
        "```Your Answer Here```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9-5hYIfqDH0n"
      },
      "source": [
        "## Q2: Fine-tuning using LoRa (75 pts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u8HEmbK_8MdD"
      },
      "source": [
        "Let's make it more interesting. We certainly don't want to just prompt models here. We will fine-tune a base model using a small classification dataset on emotion detection. The resulting model's performance will be compared with the instruction-tuned model by Meta and the base model. We will get a sense of how everything works quantitively. We don't want you to just stare at the screen watching the model converge. With the right configurations, your training should not take more than 10 minutes and the purpose here is for you to learn a diverse set of tools that will help you in doing your final project."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j3UPtgJOR70R"
      },
      "source": [
        "### A. Dataset (15 pts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nXdnYXdjVW3L"
      },
      "outputs": [],
      "source": [
        "DS_NAME = 'emotion'\n",
        "DS_TRAINING_SIZE = 1500\n",
        "DS_TEST_SIZE = 100\n",
        "DS_VALIDATION_SIZE = 50"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XbvMXG5p8MdD"
      },
      "source": [
        "a. Read the dataset from huggingface. Look at the features and the distribution on the labels of the dataset to get a sense of what it is about."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-03-03T16:40:41.312906Z",
          "iopub.status.busy": "2025-03-03T16:40:41.312623Z",
          "iopub.status.idle": "2025-03-03T16:40:41.316143Z",
          "shell.execute_reply": "2025-03-03T16:40:41.315379Z",
          "shell.execute_reply.started": "2025-03-03T16:40:41.312885Z"
        },
        "id": "GTqcoYVM8MdD",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "### Your Code Here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GAWrn5xm8MdD"
      },
      "source": [
        "#### Q2.0: Utilities (5 pts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6TzKIAKZ8MdD"
      },
      "source": [
        "a. Write a function named `get_stratified_sample` that takes the following parameters:\n",
        "- `dataset`: The input dataset (a Hugging Face Dataset object).\n",
        "- `n_samples`: The desired number of samples in the stratified sample.\n",
        "- `random_state`: An integer for reproducible sampling (default to 42).\n",
        "\n",
        "The function should return a stratified sample of the dataset, maintaining the original class proportions.\n",
        "\n",
        "Keep in mind that we need ```DS_TRAINING_SIZE``` samples for training and ```DS_TEST_SIZE``` samples for testing. If you are going to use the validation set, ```DS_VALIDATION_SIZE``` is needed for this. You may change these if you see fit but with these numbers, you can get a good enough result in an acceptable time.\n",
        "\n",
        "***NOTE:*** Make sure your function shuffles the final dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-03-03T16:40:41.317322Z",
          "iopub.status.busy": "2025-03-03T16:40:41.317002Z",
          "iopub.status.idle": "2025-03-03T16:40:41.328144Z",
          "shell.execute_reply": "2025-03-03T16:40:41.327480Z",
          "shell.execute_reply.started": "2025-03-03T16:40:41.317302Z"
        },
        "id": "I7L5GmVO8MdD",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "### Your Code Here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YmAsf8d4wAnS"
      },
      "source": [
        "b. Use your function to create train,test and (validation) sets. Compare the distribution of labels with the full dataset to make sure it's working correctly. Printing or plotting the distributions is enough."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-03-03T16:40:44.027694Z",
          "iopub.status.busy": "2025-03-03T16:40:44.027478Z",
          "iopub.status.idle": "2025-03-03T16:40:44.037507Z",
          "shell.execute_reply": "2025-03-03T16:40:44.036588Z",
          "shell.execute_reply.started": "2025-03-03T16:40:44.027676Z"
        },
        "id": "hxMY1ac88MdD",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "### Your Code Here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YiZX6nlULxUe"
      },
      "source": [
        "#### Q2.1: Preparing Data for Fine-Tuning (10 pts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6emefGZV8MdD"
      },
      "source": [
        "a. Let's get the emotion dataset ready for LoRA fine-tuning. Here's what you need to do:\n",
        "\n",
        "1.  **Format the Data**: Turn each data entry into a conversation like this:\n",
        "    *   A system instruction that tells the model what to do (analyze emotions)```*```.\n",
        "    *   A user query that gives the model the text to analyze.\n",
        "    *   An assistant response that provides the correct emotion label (in natural language, naturally!)\n",
        "2.  **Tokenize and Label**:\n",
        "    *   Tokenize the formatted conversation.\n",
        "    *   Prepare labels for training, make sure to mask the instruction part of the data ```**```.\n",
        "\n",
        "Also, write a verification function that in a human readable format:\n",
        "\n",
        "*   Prints the complete training input sequence after tokenization for a given data entry.\n",
        "*   Shows the labels, indicating which tokens are being predicted.\n",
        "*   Checks if the assistant header is correctly handled by finding its position in the text and printing the subsequent text.\n",
        "\n",
        "```*TIP:``` It is a good practice to make your system instruction as concise as possible. For example in this task, you should tell the LLM explicitly that what are the valid labels."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hDtRf-Bx8MdD"
      },
      "source": [
        "b. When preparing the data, experiment with the tokenizer parameters, namely `truncation`, `padding` and `max_length`. In a ```concise``` manner, explain what each one of them does and what is a good value and why."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-BBTOIWywisS"
      },
      "source": [
        "```Your Answer Here```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7hVKUSxK8MdD"
      },
      "source": [
        "c. ```**```When preparing the data, mask the instruction part of the data (set labels to -100 for the instruction tokens) before starting the training. Why is this a good idea?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Mverm_SwloH"
      },
      "source": [
        "```Your Answer Here```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-03-03T16:40:44.038922Z",
          "iopub.status.busy": "2025-03-03T16:40:44.038499Z",
          "iopub.status.idle": "2025-03-03T16:40:44.055783Z",
          "shell.execute_reply": "2025-03-03T16:40:44.054965Z",
          "shell.execute_reply.started": "2025-03-03T16:40:44.038886Z"
        },
        "id": "gkwNaJCY8MdD",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "### Your Code Here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_6BnCCXC8MdE"
      },
      "source": [
        "d. Run your verification function on the first sample of your training dataset to see everything is in order."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-03-03T16:40:49.910892Z",
          "iopub.status.busy": "2025-03-03T16:40:49.910602Z",
          "iopub.status.idle": "2025-03-03T16:40:49.914663Z",
          "shell.execute_reply": "2025-03-03T16:40:49.913803Z",
          "shell.execute_reply.started": "2025-03-03T16:40:49.910862Z"
        },
        "id": "WySUgurB8MdE",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "### Your Code Here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IbkYwTvAR70S"
      },
      "source": [
        "### B. Fine-tune using LoRa (30 pts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-03-03T16:40:49.915681Z",
          "iopub.status.busy": "2025-03-03T16:40:49.915495Z",
          "iopub.status.idle": "2025-03-03T16:40:49.928220Z",
          "shell.execute_reply": "2025-03-03T16:40:49.927411Z",
          "shell.execute_reply.started": "2025-03-03T16:40:49.915661Z"
        },
        "id": "v6aLrrmnR70S",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "\n",
        "def print_trainable_parameters(model):\n",
        "    \"\"\"\n",
        "    Prints the number of trainable parameters in the model.\n",
        "    \"\"\"\n",
        "    trainable_params = 0\n",
        "    all_param = 0\n",
        "    for _, param in model.named_parameters():\n",
        "        all_param += param.numel()\n",
        "        if param.requires_grad:\n",
        "            trainable_params += param.numel()\n",
        "    print(\n",
        "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OmYqjqIJ8MdE"
      },
      "source": [
        "#### Q2.2: Experimenting with LoRA Configuration Parameters (3 pts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CxVKOKLP8MdE"
      },
      "source": [
        "In this section, you may explore the effect of different LoRA configuration parameters on the trainable parameter count:\n",
        "\n",
        "1. Try different rank values (`r`) - experiment with values like 8, 16, 32, and 64\n",
        "    - Higher rank allows for more expressive power but increases parameter count\n",
        "    \n",
        "2. Adjust the scaling factor (`lora_alpha`) - typically set to 2x the rank\n",
        "    - This affects the magnitude of updates during training\n",
        "    \n",
        "3. Modify target modules - test different combinations like:\n",
        "    - Only attention modules: `[\"q_proj\", \"v_proj\"]`\n",
        "    - All attention modules: `[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"]`\n",
        "    - Including feed-forward: `[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"]`\n",
        "    \n",
        "4. Vary dropout rates (`lora_dropout`) - test values like 0.0, 0.05, 0.1\n",
        "    - Higher dropout can help with regularization\n",
        "\n",
        "You may use the `print_trainable_parameters()` function to observe how each change affects the number of trainable parameters.\n",
        "\n",
        "(We are not requiring you to print and explain everything, these are some values to help you out)\n",
        "\n",
        "a. Find a configuration that provides a good balance between parameter efficiency and model expressiveness. Explain your reasons in a concise manner.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "78-QYYMRx1kx"
      },
      "source": [
        "```Your Answer Here```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-03-03T16:40:49.929399Z",
          "iopub.status.busy": "2025-03-03T16:40:49.929040Z",
          "iopub.status.idle": "2025-03-03T16:40:49.938778Z",
          "shell.execute_reply": "2025-03-03T16:40:49.937950Z",
          "shell.execute_reply.started": "2025-03-03T16:40:49.929370Z"
        },
        "id": "b42nIsQs8MdE",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "### Your Answer Here (Final Chosen Lora Config + Output of trainable parameters function on that)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9sqV1RFl8MdE"
      },
      "source": [
        "#### Q2.3: Training Callbacks and Early Stopping (10 pts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xHORrpBU8MdE"
      },
      "source": [
        "\n",
        "**Understanding Training Callbacks**\n",
        "\n",
        "Generally speaking, in deep learning, callbacks are functions that can be applied at various stages of training\n",
        "(start/end of training, epoch, or batch) to modify the training process. They're powerful\n",
        "tools that allow you to:\n",
        "\n",
        "- Monitor training metrics in real-time\n",
        "- Add custom logging\n",
        "- Save model checkpoints\n",
        "- Implement early stopping\n",
        "- Adjust learning rates dynamically\n",
        "\n",
        "**Early Stopping**\n",
        "\n",
        "Early stopping is a regularization technique that prevents overfitting by stopping training\n",
        "when a monitored metric stops improving. Benefits include:\n",
        "\n",
        "- Reduced training time\n",
        "- Better generalization\n",
        "- Prevention of overfitting\n",
        "\n",
        "**Your Task**\n",
        "\n",
        "a. Implement a custom callback class that:\n",
        "1. Tracks the best loss value during training\n",
        "2. Calculates perplexity in steps\n",
        "3. Adds perplexity to the training logs\n",
        "4. Implements early stopping if the loss doesn't improve for several steps (This is called patience)\n",
        "5. (In your final project it is a good idea to use the big enough validation set to better monitor the training process. Given the time constraints for this assignment, we are not requiring you to do that.)\n",
        "\n",
        "***NOTE:*** You should inherit from the TrainerCallback class implemented in transformers\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-03-03T16:40:50.247620Z",
          "iopub.status.busy": "2025-03-03T16:40:50.247372Z",
          "iopub.status.idle": "2025-03-03T16:40:50.250959Z",
          "shell.execute_reply": "2025-03-03T16:40:50.250043Z",
          "shell.execute_reply.started": "2025-03-03T16:40:50.247594Z"
        },
        "id": "qSuqk1iT8MdE",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "### Your Code Here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b4jDQHHI8MdE"
      },
      "source": [
        "#### Q2.4: TrainingArgs (7 pts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t9z_s_di8MdE"
      },
      "source": [
        "b. Explain the purpose of a minimum of 5 of the following TrainingArguments parameters in ```at most two sentences.```\n",
        "For each parameter, suggest a good value for our emotion classification problem,\n",
        "considering we are using a Llama-3.2-1B model and training in a Colab/Kaggle environment.\n",
        "Explain why you chose that value.\n",
        "\n",
        "1.  `lr_scheduler_type`\n",
        "2.  `per_device_train_batch_size`\n",
        "3.  `gradient_accumulation_steps`\n",
        "4.  `learning_rate`\n",
        "5.  `weight_decay`\n",
        "6.  `bf16`\n",
        "7.  `max_grad_norm`\n",
        "8.  `warmup_ratio`\n",
        "9.  `group_by_length`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-WCOZSury8Nb"
      },
      "source": [
        "```Your Answer Here```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f7mjEtzp8MdE"
      },
      "source": [
        "b. Define your trainings args"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-03-03T16:40:51.494861Z",
          "iopub.status.busy": "2025-03-03T16:40:51.494560Z",
          "iopub.status.idle": "2025-03-03T16:40:51.498158Z",
          "shell.execute_reply": "2025-03-03T16:40:51.497280Z",
          "shell.execute_reply.started": "2025-03-03T16:40:51.494838Z"
        },
        "id": "xKEXnRnV8MdE",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "### Your Code Here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oesSgQLnleqH"
      },
      "source": [
        "#### Q2.5: Memory usage (8 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fj_Fk88wlvbN"
      },
      "source": [
        "Now, we want to determine the memory required to **load and train** the LLM in different fine-tuning scenarios.  \n",
        "\n",
        "- **Full Fine-Tuning:** Calculate the total memory needed when updating all model parameters.  \n",
        "- **LoRA Fine-Tuning:** Calculate the memory needed based on your LoRA configuration.  \n",
        "- Use your current settings for the calculations.  \n",
        "- Refer to [this resource](https://blog.eleuther.ai/transformer-math/) for guidance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QVmS5BKyoj0p"
      },
      "source": [
        "```Your Answer Here```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YYso3Xrn8MdF"
      },
      "source": [
        "#### Q2.6: Training the model (2 pts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qV_wnGtE8MdF"
      },
      "source": [
        "Train and save the model. Your training should take at most 10 minutes on a Google colab notebook.\n",
        "\n",
        "***PRO-TIP:*** If you want to go a step further on a good training task, you may research and use model checkpointing and monitoring tools (like weights and biases and tensorboard) But it's not required here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-03-03T16:40:51.548439Z",
          "iopub.status.busy": "2025-03-03T16:40:51.548145Z",
          "iopub.status.idle": "2025-03-03T16:40:51.551894Z",
          "shell.execute_reply": "2025-03-03T16:40:51.550931Z",
          "shell.execute_reply.started": "2025-03-03T16:40:51.548409Z"
        },
        "id": "b_jJc05S8MdF",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "### Your Code Here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vyY2GI_JTw3r"
      },
      "source": [
        "### C. Some other PEFT methods (6 pts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HosLXDArT7MT"
      },
      "source": [
        "#### Q2.7: IA3 method (2 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fz5hBZTedXc2"
      },
      "source": [
        "IA3 ([Liu et al., 2022](https://openreview.net/pdf?id=rBCvMG-JsPd)) is another PEFT method. Briefly explain how it works."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4oi-m6r2c9kW"
      },
      "source": [
        "```Your Answer Here```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HUnIbezUr8Qo"
      },
      "source": [
        "#### Q2.8: Soft Prompt methods (4 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZEoBfkeNz0hd"
      },
      "source": [
        "Instead of fine-tuning all model parameters, prompting uses additional input text to guide a frozen model toward a specific task.  \n",
        "\n",
        "There are two types of prompts [(Hugging Face, PEFT)](https://huggingface.co/docs/peft/en/conceptual_guides/prompting):  \n",
        "- **Hard prompts**: Manually crafted text prompts using discrete tokens, but designing them is labor-intensive.  \n",
        "- **Soft prompts**: Learnable tensors concatenated with input embeddings and optimized for a dataset, but they are not human-readable.  \n",
        "\n",
        "In this section, you will explore how soft prompts are implemented and fine-tuned using PEFT.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8csZKx8is7y-"
      },
      "source": [
        "Briefly explain the following soft prompt methods and highlight their key differences:  \n",
        "- **Prompt Tuning** [(Lester et al., 2021)](https://aclanthology.org/2021.emnlp-main.243.pdf)  \n",
        "- **Prefix Tuning** [(Li & Liang, 2021)](https://aclanthology.org/2021.acl-long.353.pdf)  \n",
        "- **P-Tuning** [(Liu et al., 2021)](https://arxiv.org/pdf/2103.10385)  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uNEv3hjgiJJC"
      },
      "source": [
        "```Your Answer Here```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1grzEyrLR70S"
      },
      "source": [
        "### D. Evaluate and Comparison (24 pts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ilTEDObKftKW"
      },
      "source": [
        "#### Q2.9: Generating Output from Models (10 pts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rv2Fo5R18MdF"
      },
      "source": [
        "Generate the output of models on the task of emotion detection using:\n",
        "\n",
        "- LoRa fine-tuned Model by you\n",
        "- Instruction tuned model by Meta\n",
        "- Base model by Meta\n",
        "\n",
        "You may use ```Regex``` or simply looking for label names in model outputs to do obtain the classification repots. Looking at the results generated by models can help you greatly to find the best way to parse the output.\n",
        "\n",
        "***NOTE:*** Your fine-tuned model MUST outperform the base model, but outperforming the instruction tuned model is optional and has extra points. (5 pts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PM6Q7gg90Frt"
      },
      "outputs": [],
      "source": [
        "### Your Code Here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KYsPWxmJftKW"
      },
      "source": [
        "#### Q2.10: Performance Comparison Visualization (4 pts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vIsVZU3Q0Hq7"
      },
      "source": [
        "Compare the Accuracy and Micro-F1 in a grouped bar chart. (4 pts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dMNV7_180RAC"
      },
      "outputs": [],
      "source": [
        "### Your Code Here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pvgCr2cYftKW"
      },
      "source": [
        "#### Q2.11: Analysis (10 pts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jd5DZCWA0dkQ"
      },
      "source": [
        "Analyze the results and the reasons behind them."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MFwho05S0nLv"
      },
      "source": [
        "```Your Answer Here```"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [],
      "dockerImageVersionId": 30919,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}